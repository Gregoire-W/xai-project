{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9f626f",
   "metadata": {},
   "source": [
    "# Fine-Tuning Service for Transfer Learning\n",
    "\n",
    "This notebook contains utility functions to fine-tune pre-trained Keras models for binary classification tasks. It implements a two-phase training strategy: first training only the classifier head, then fine-tuning the entire model with unfrozen layers.\n",
    "\n",
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b3467-af76-47b4-ba23-ff698564f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# deep learning\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7360c",
   "metadata": {},
   "source": [
    "## Function 1: Create Datasets\n",
    "\n",
    "**Purpose**: Load and prepare training, validation, and test datasets from directories.\n",
    "\n",
    "**Why**: TensorFlow's `image_dataset_from_directory` automatically loads images from folder structure, applies batching, and prefetching for optimal performance during training.\n",
    "\n",
    "**Key features**:\n",
    "- Automatically infers labels from subdirectory names\n",
    "- Applies shuffling for training/validation to prevent overfitting\n",
    "- Uses prefetching to overlap data loading with model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df24bf-a811-47d8-9b85-e6bd105c3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(train_path, val_path, test_path, batch_size, img_size):\n",
    "\n",
    "    train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_path,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        image_size=img_size\n",
    "    )\n",
    "\n",
    "    validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_path,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        image_size=img_size\n",
    "    )\n",
    "\n",
    "    test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_path,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        image_size=img_size\n",
    "    )\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    \n",
    "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00ea89",
   "metadata": {},
   "source": [
    "## Function 2: Create Model\n",
    "\n",
    "**Purpose**: Build the initial transfer learning model with a frozen base model and new classification head.\n",
    "\n",
    "**Why**: We freeze the pre-trained base model to preserve its learned features from ImageNet while training only the new classification layers. This prevents destroying the valuable pre-trained weights during initial training.\n",
    "\n",
    "**Architecture**:\n",
    "1. Pre-trained base model (frozen)\n",
    "2. Global Average Pooling layer (reduces spatial dimensions)\n",
    "3. Dropout layer (prevents overfitting)\n",
    "4. Dense layer with sigmoid activation (binary classification)\n",
    "\n",
    "**Returns**: The complete model, plus references to the pooling and prediction layers (needed later for GradCAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f67ad2-8710-4f3b-ad79-77ec32e052f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(base_model, pre_process, dropout):\n",
    "    \n",
    "    base_model.trainable = False\n",
    "\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "    prediction_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3), name=\"img_input_finetune\")\n",
    "    x = pre_process(inputs)\n",
    "    x = base_model(x, training=False)\n",
    "    x = global_average_layer(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model, global_average_layer, prediction_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015849f",
   "metadata": {},
   "source": [
    "## Function 3: Train Classifier\n",
    "\n",
    "**Purpose**: Train only the classification head while keeping the base model frozen (Phase 1 of transfer learning).\n",
    "\n",
    "**Why**: This warm-up phase allows the new classification layers to adapt to our specific task without disrupting the pre-trained features. We evaluate initial performance, then train with checkpointing to save progress at each epoch.\n",
    "\n",
    "**Key aspects**:\n",
    "- Uses Adam optimizer with a standard learning rate\n",
    "- Monitors accuracy and AUC metrics\n",
    "- Saves a checkpoint after each epoch for later selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972c1323-9edb-42d8-a4c9-135ed6d59062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, lr, epochs, train_dataset, validation_dataset, cp_path):\n",
    "    base_learning_rate = lr\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.AUC()\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    initial_epochs = epochs\n",
    "    \n",
    "    loss0, accuracy0, auc0 = model.evaluate(validation_dataset)\n",
    "\n",
    "    print(\"initial loss: {:.2f}\".format(loss0))\n",
    "    print(\"initial accuracy: {:.2f}\".format(accuracy0))\n",
    "    print(\"initial auc: {:.2f}\".format(auc0))\n",
    "        \n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=str(cp_path / \"model_epoch_{epoch:02d}.keras\"),\n",
    "        save_weights_only=False,\n",
    "        save_freq=\"epoch\",\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=initial_epochs,\n",
    "        validation_data=validation_dataset,\n",
    "        callbacks=[checkpoint_cb]\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e262b2",
   "metadata": {},
   "source": [
    "## Function 4: Plot Metrics\n",
    "\n",
    "**Purpose**: Visualize training and validation metrics over epochs to assess model performance and detect overfitting.\n",
    "\n",
    "**Why**: Plotting accuracy and loss curves helps us understand if the model is learning properly, if it's overfitting (validation metrics diverging from training), and when training should stop.\n",
    "\n",
    "**Displays**:\n",
    "- Training vs. Validation Accuracy (should increase together)\n",
    "- Training vs. Validation Loss (should decrease together)\n",
    "\n",
    "**Returns**: Updated metric lists for cumulative plotting across multiple training phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b127a5e1-3a9c-4177-ad16-b9215e2a2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history, acc, val_acc, loss, val_loss, auc, val_auc):\n",
    "    auc_keys = [elem for elem in history.history.keys() if \"auc\" in elem]\n",
    "    val_auc_key = [elem for elem in auc_keys if elem.startswith(\"val\")][0]\n",
    "    auc_key = [elem for elem in auc_keys if not elem.startswith(\"val\")][0]\n",
    "    \n",
    "    acc += history.history[\"accuracy\"]\n",
    "    val_acc += history.history[\"val_accuracy\"]\n",
    "    auc += history.history[auc_key] \n",
    "    \n",
    "    loss += history.history[\"loss\"]\n",
    "    val_loss += history.history[\"val_loss\"]\n",
    "    val_auc += history.history[val_auc_key]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label=\"Training Accuracy\")\n",
    "    plt.plot(val_acc, label=\"Validation Accuracy\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label=\"Training Loss\")\n",
    "    plt.plot(val_loss, label=\"Validation Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.ylabel(\"Cross Entropy\")\n",
    "    plt.ylim([0,1.0])\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.show()\n",
    "\n",
    "    return acc, val_acc, loss, val_loss, auc, val_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd062e",
   "metadata": {},
   "source": [
    "## Function 5: Get Best Checkpoint\n",
    "\n",
    "**Purpose**: Evaluate all saved checkpoints and select the one with the best test accuracy.\n",
    "\n",
    "**Why**: Models can overfit during training, so the last epoch isn't always the best. By evaluating all checkpoints, we can select the model that generalizes best to unseen data while monitoring the validation-test gap to detect overfitting.\n",
    "\n",
    "**Process**:\n",
    "- Load each checkpoint\n",
    "- Evaluate on test and validation sets\n",
    "- Compare performance and select the best\n",
    "- Return the loaded best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06139cdc-9115-409c-b0b9-f8011f04063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_cp(cp_path, test_dataset, validation_dataset):\n",
    "    results = []\n",
    "    for model_path in sorted(cp_path.iterdir()):\n",
    "        checkpoint = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Test accuracy\n",
    "        _, test_acc, _ = checkpoint.evaluate(test_dataset, verbose=0)\n",
    "        \n",
    "        # Validation accuracy\n",
    "        _, val_accuracy, _ = checkpoint.evaluate(validation_dataset, verbose=0)\n",
    "        \n",
    "        results.append({\n",
    "            \"model_path\": model_path,\n",
    "            \"epoch\": model_path.name,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"gap\": val_accuracy - test_acc\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_path.name:25} | Test: {test_acc:.4f} | Val: {val_accuracy:.4f} | Gap: {val_accuracy - test_acc:.4f}\")\n",
    "    \n",
    "    best = max(results, key=lambda x: x['test_acc'])\n",
    "    print(f\"\\nBest checkpoint: {best['epoch']} with test accuracy = {best['test_acc']:.4f}\")\n",
    "\n",
    "    return tf.keras.models.load_model(best[\"model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9adb38",
   "metadata": {},
   "source": [
    "## Function 6: Fine-Tune Classifier\n",
    "\n",
    "**Purpose**: Unfreeze and retrain the top layers of the base model (Phase 2 of transfer learning).\n",
    "\n",
    "**Why**: After the classification head has adapted to our task, we can carefully unfreeze some base model layers and train them with a much lower learning rate (10-100x lower). This allows the model to adapt pre-trained features to our specific domain without catastrophic forgetting.\n",
    "\n",
    "**Strategy**:\n",
    "- Unfreeze layers from `fine_tune_at` onwards\n",
    "- Use RMSprop optimizer with reduced learning rate\n",
    "- Continue training from the previous checkpoint\n",
    "- Save new checkpoints for final selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70d25127-f44b-469b-a939-109fb45f20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_classifier(model, base_model, train_dataset, validation_dataset, fine_tune_at, lr, cp_path, initial_epochs, epochs):\n",
    "    base_model.trainable = True\n",
    "\n",
    "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "    \n",
    "    # Freeze all the layers before the 'fine_tune_at' layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "      layer.trainable = False\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr),\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.AUC()\n",
    "        ],\n",
    "    )\n",
    "    print(\"Number of trainable variables in the model: \", len(model.trainable_variables))\n",
    "    \n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=str(cp_path / \"model_epoch_{epoch:02d}.keras\"),\n",
    "        save_weights_only=False,\n",
    "        save_freq=\"epoch\",\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    fine_tune_epochs = epochs\n",
    "    total_epochs =  initial_epochs + fine_tune_epochs\n",
    "    \n",
    "    \n",
    "    history_fine = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=total_epochs,\n",
    "        initial_epoch=initial_epochs,\n",
    "        validation_data=validation_dataset,\n",
    "        callbacks=[checkpoint_cb]\n",
    "    )\n",
    "\n",
    "    return model, history_fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8cb20",
   "metadata": {},
   "source": [
    "## Function 7: Create GradCAM Model\n",
    "\n",
    "**Purpose**: Build a specialized model for generating GradCAM heatmaps to visualize which image regions the model focuses on.\n",
    "\n",
    "**Why**: GradCAM requires access to both the last convolutional layer's activations and the final predictions. This function creates a new model that outputs both, enabling us to compute gradients and generate explanation heatmaps.\n",
    "\n",
    "**Output**: A model that returns `[last_conv_activations, predictions]` when called, which is exactly what the GradCAM algorithm needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8401714c-5b4c-4bc8-b4c6-43775d03b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grad_cam_model(last_conv_layer_name, base_model, global_average_layer, prediction_layer):\n",
    "\n",
    "    last_conv_layer = base_model.get_layer(last_conv_layer_name)\n",
    "    \n",
    "    base_model_output = base_model.output\n",
    "    x = global_average_layer(base_model_output)\n",
    "\n",
    "    final_predictions = prediction_layer(x)\n",
    "    \n",
    "    grad_model = tf.keras.Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=[last_conv_layer.output, final_predictions]\n",
    "    )\n",
    "    \n",
    "    return grad_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2afbd93",
   "metadata": {},
   "source": [
    "## Function 8: Main Fine-Tune Pipeline\n",
    "\n",
    "**Purpose**: Orchestrate the complete fine-tuning workflow from data loading to final model export.\n",
    "\n",
    "**Why**: This is the main entry point that combines all previous functions into a complete pipeline. It handles directory management, executes both training phases, selects the best checkpoints, and saves all necessary artifacts.\n",
    "\n",
    "**Complete workflow**:\n",
    "1. Create checkpoint directories\n",
    "2. Load datasets\n",
    "3. Build initial model\n",
    "4. **Phase 1**: Train classifier head (frozen base)\n",
    "5. Select best classifier checkpoint\n",
    "6. **Phase 2**: Fine-tune with unfrozen layers\n",
    "7. Select best fine-tuned checkpoint\n",
    "8. Create and save GradCAM model\n",
    "9. Save all models (classifier, fine-tuned, gradcam)\n",
    "\n",
    "**Saves three models**:\n",
    "- `classifier.keras`: Best model after Phase 1\n",
    "- `fine_tuned.keras`: Best model after Phase 2 (use this for predictions)\n",
    "- `grad_cam.keras`: GradCAM model for explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633dd7aa-6aa0-4d58-a6d1-b19bf28c6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(\n",
    "    train_path,\n",
    "    val_path,\n",
    "    test_path,\n",
    "    batch_size,\n",
    "    img_size,\n",
    "    base_model,\n",
    "    base_model_layer_name,\n",
    "    pre_process,\n",
    "    dropout,\n",
    "    lr_classifier,\n",
    "    epochs_classifier,\n",
    "    cp_path,\n",
    "    fine_tune_at,\n",
    "    lr_finetune,\n",
    "    epochs_finetune,\n",
    "    model_save_path,\n",
    "    last_conv_layer_name,\n",
    "):\n",
    "    acc, val_acc, loss, val_loss, auc, val_auc = [], [], [], [], [], []\n",
    "    classifier_cp_path = cp_path / \"classifier\"\n",
    "    fine_tuned_cp_path = cp_path / \"fine_tuned\"\n",
    "\n",
    "    # check path exists\n",
    "    if not classifier_cp_path.exists():\n",
    "        classifier_cp_path.mkdir(parents=True, exist_ok=True)\n",
    "    if not fine_tuned_cp_path.exists():\n",
    "        fine_tuned_cp_path.mkdir(parents=True, exist_ok=True)\n",
    "    if not model_save_path.exists():\n",
    "        model_save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_dataset, validation_dataset, test_dataset = create_datasets(train_path, val_path, test_path, batch_size, img_size)\n",
    "\n",
    "    model, global_average_layer, prediction_layer = create_model(base_model, pre_process, dropout)\n",
    "\n",
    "    print(\"Start training classifier\")\n",
    "    model, history = train_classifier(model, lr_classifier, epochs_classifier, train_dataset, validation_dataset, classifier_cp_path)\n",
    "    print(f\"metrics: {history.history.keys()}\")\n",
    "\n",
    "    acc, val_acc, loss, val_loss, auc, val_auc = plot_metrics(history, acc, val_acc, loss, val_loss, auc, val_auc)\n",
    "\n",
    "    print(\"Let's select the best checkpoint based on accuracy:\")\n",
    "    model = get_best_cp(classifier_cp_path, test_dataset, validation_dataset)\n",
    "    model.save(str(model_save_path / \"classifier.keras\"))\n",
    "    base_model = model.get_layer(base_model_layer_name)\n",
    "\n",
    "    print(\"Start finetuning classifier\")\n",
    "    model, history = fine_tune_classifier(model, base_model, train_dataset, validation_dataset, fine_tune_at, lr_finetune, fine_tuned_cp_path, epochs_classifier, epochs_finetune)\n",
    "\n",
    "    acc, val_acc, loss, val_loss, auc, val_auc = plot_metrics(history, acc, val_acc, loss, val_loss, auc, val_auc)\n",
    "\n",
    "    print(\"Let's select the best checkpoint based on accuracy:\")\n",
    "    model = get_best_cp(fine_tuned_cp_path, test_dataset, validation_dataset)\n",
    "    model.save(str(model_save_path / \"fine_tuned.keras\"))\n",
    "\n",
    "    grad_cam = create_grad_cam_model(last_conv_layer_name, base_model, global_average_layer, prediction_layer)\n",
    "    grad_cam.save(str(model_save_path / \"grad_cam.keras\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
